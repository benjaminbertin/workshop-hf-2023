{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Installation de Transformers\n",
    "! pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "torch.__version__, transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tour d'horizon de ü§ó Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soyez op√©rationnel avec ü§ó Transformers ! Que vous soyez un d√©veloppeur ou un utilisateur lambda, cette visite rapide vous aidera √† d√©marrer et vous montrera comment utiliser le `pipeline()` pour l'inf√©rence, charger un mod√®le pr√©-entra√Æn√© et un pr√©processeur avec une [AutoClass](https://huggingface.co/docs/transformers/main/fr/./model_doc/auto), et entra√Æner rapidement un mod√®le avec PyTorch ou TensorFlow. Si vous √™tes un d√©butant, nous vous recommandons de consulter nos tutoriels ou notre [cours](https://huggingface.co/course/chapter1/1) suivant pour des explications plus approfondies des concepts pr√©sent√©s ici.\n",
    "\n",
    "Avant de commencer, assurez-vous que vous avez install√© toutes les biblioth√®ques n√©cessaires :\n",
    "\n",
    "```bash\n",
    "!pip install transformers datasets\n",
    "```\n",
    "\n",
    "Vous aurez aussi besoin d'installer votre biblioth√®que d'apprentissage profond favorite :\n",
    "\n",
    "```bash\n",
    "pip install torch\n",
    "```\n",
    "```bash\n",
    "pip install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le `pipeline()` est le moyen le plus simple d'utiliser un mod√®le pr√©-entra√Æn√© pour l'inf√©rence. Vous pouvez utiliser le `pipeline()` pr√™t √† l'emploi pour de nombreuses t√¢ches dans diff√©rentes modalit√©s. Consultez le tableau ci-dessous pour conna√Ætre les t√¢ches prises en charge :\n",
    "\n",
    "| **T√¢che**                     | **Description**                                                                                              | **Modalit√©**        | **Identifiant du pipeline**                   |\n",
    "|------------------------------|--------------------------------------------------------------------------------------------------------------|----------------------|-----------------------------------------------|\n",
    "| Classification de texte      | Attribue une cat√©gorie √† une s√©quence de texte donn√©e                                                        | Texte                | pipeline(task=\"sentiment-analysis\")           |\n",
    "| G√©n√©ration de texte          | G√©n√®re du texte √† partir d'une consigne donn√©e                                                               | Texte                | pipeline(task=\"text-generation\")              |\n",
    "| Reconnaissance de token nomm√©      | Attribue une cat√©gorie √† chaque token dans une s√©quence (personnes, organisation, localisation, etc.)                            | Texte                | pipeline(task=\"ner\")                          |\n",
    "| Question r√©ponse             | Extrait une r√©ponse du texte en fonction du contexte et d'une question                                       | Texte                | pipeline(task=\"question-answering\")           |\n",
    "| Pr√©diction de token masqu√©                    | Pr√©dit correctement le token masqu√© dans une s√©quence                                                               | Texte                | pipeline(task=\"fill-mask\")                    |\n",
    "| G√©n√©ration de r√©sum√©                | G√©n√®re un r√©sum√© d'une s√©quence de texte donn√©e ou d'un document                                                         | Texte                | pipeline(task=\"summarization\")                |\n",
    "| Traduction                  | Traduit du texte d'un langage √† un autre                                                                      | Texte                | pipeline(task=\"translation\")                  |\n",
    "| Classification d'image       | Attribue une cat√©gorie √† une image                                                                           | Image                | pipeline(task=\"image-classification\")         |\n",
    "| Segmentation d'image           | Attribue une cat√©gorie √† chaque pixel d'une image (supporte la segmentation s√©mantique, panoptique et d'instance) | Image                | pipeline(task=\"image-segmentation\")           |\n",
    "| D√©tection d'objects             | Pr√©dit les d√©limitations et cat√©gories d'objects dans une image                                                | Image                | pipeline(task=\"object-detection\")             |\n",
    "| Classification d'audio       | Attribue une cat√©gorie √† un fichier audio                                                                    | Audio                | pipeline(task=\"audio-classification\")         |\n",
    "| Reconnaissance automatique de la parole | Extrait le discours d'un fichier audio en texte                                                                  | Audio                | pipeline(task=\"automatic-speech-recognition\") |\n",
    "| Question r√©ponse visuels    | Etant donn√©es une image et une question, r√©pond correctement √† une question sur l'image                                   | Modalit√©s multiples  | pipeline(task=\"vqa\")                          |\n",
    "\n",
    "Commencez par cr√©er une instance de `pipeline()` et sp√©cifiez la t√¢che pour laquelle vous souhaitez l'utiliser. Vous pouvez utiliser le `pipeline()` pour n'importe laquelle des t√¢ches mentionn√©es dans le tableau pr√©c√©dent. Pour obtenir une liste compl√®te des t√¢ches prises en charge, consultez la documentation de l'[API pipeline](https://huggingface.co/docs/transformers/main/fr/./main_classes/pipelines). Dans ce guide, nous utiliserons le `pipeline()` pour l'analyse des sentiments √† titre d'exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le `pipeline()` t√©l√©charge et stocke en cache un [mod√®le pr√©-entra√Æn√©](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) et un tokenizer par d√©faut pour l'analyse des sentiments. Vous pouvez maintenant utiliser le `classifier` sur le texte de votre choix :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(\"We are very happy to show you the ü§ó Transformers library.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous voulez classifier plus qu'un texte, donnez une liste de textes au `pipeline()` pour obtenir une liste de dictionnaires en retour :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = classifier([\"We are very happy to show you the ü§ó Transformers library.\", \"We hope you don't hate it.\"])\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, avec le score de: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le `pipeline()` peut aussi it√©rer sur un jeu de donn√©es entier pour n'importe quelle t√¢che. Prenons par exemple un jeu de donn√©es d'avis sur IMDB (voir le ü§ó Datasets [Quick Start](https://huggingface.co/docs/datasets/quickstart#audio) pour plus de d√©tails):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objet retourn√© se comporte comme une liste de dictionnaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[5]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[5]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons passer ce dataset comme une liste au pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = classifier(dataset[:6][\"text\"])\n",
    "print([d[\"label\"] for d in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les ensembles de donn√©es plus importants o√π les entr√©es sont volumineuses (comme dans les domaines de la parole ou de la vision), utilisez plut√¥t un g√©n√©rateur au lieu d'une liste pour charger toutes les entr√©es en m√©moire. Pour plus d'informations, consultez la documentation de l'[API pipeline](https://huggingface.co/docs/transformers/main/fr/./main_classes/pipelines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utiliser une autre mod√®le et tokenizer dans le pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le `pipeline()` peut √™tre utilis√© avec n'importe quel mod√®le du [Hub](https://huggingface.co/models), ce qui permet d'adapter facilement le `pipeline()` √† d'autres cas d'utilisation. Par exemple, si vous souhaitez un mod√®le capable de traiter du texte fran√ßais, utilisez les filtres du Hub pour trouver un mod√®le appropri√©. Le premier r√©sultat renvoie un [mod√®le BERT](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment) multilingue finetun√© pour l'analyse des sentiments que vous pouvez utiliser pour le texte fran√ßais :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez `AutoModelForSequenceClassification` et `AutoTokenizer` pour charger le mod√®le pr√©-entra√Æn√© et le tokenizer adapt√© (plus de d√©tails sur une `AutoClass` dans la section suivante) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez `AutoModelForSequenceClassification` et `AutoTokenizer` pour charger le mod√®le pr√©-entra√Æn√© et le tokenizer adapt√© (plus de d√©tails sur une `AutoClass` dans la section suivante) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifiez le mod√®le et le tokenizer dans le `pipeline()`, et utilisez le `classifier` sur le texte en fran√ßais :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "classifier(\"Nous sommes tr√®s heureux de vous pr√©senter la biblioth√®que ü§ó Transformers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous ne parvenez pas √† trouver un mod√®le adapt√© √† votre cas d'utilisation, vous devrez finetuner un mod√®le pr√©-entra√Æn√© sur vos donn√©es. Jetez un coup d'≈ìil √† notre [tutoriel sur le finetuning](https://huggingface.co/docs/transformers/main/fr/./training) pour apprendre comment faire. Enfin, apr√®s avoir finetun√© votre mod√®le pr√©-entra√Æn√©, pensez √† [partager](https://huggingface.co/docs/transformers/main/fr/./model_sharing) le mod√®le avec la communaut√© sur le Hub afin de d√©mocratiser l'apprentissage automatique pour tous ! ü§ó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoClass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les classes `AutoModelForSequenceClassification` et `AutoTokenizer` fonctionnent ensemble pour cr√©er un `pipeline()` comme celui que vous avez utilis√© ci-dessus. Une [AutoClass](https://huggingface.co/docs/transformers/main/fr/./model_doc/auto) est un raccourci qui r√©cup√®re automatiquement l'architecture d'un mod√®le pr√©-entra√Æn√© √† partir de son nom ou de son emplacement. Il vous suffit de s√©lectionner l'`AutoClass` appropri√©e √† votre t√¢che et la classe de pr√©traitement qui lui est associ√©e. \n",
    "\n",
    "Reprenons l'exemple de la section pr√©c√©dente et voyons comment vous pouvez utiliser l'`AutoClass` pour reproduire les r√©sultats du `pipeline()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un tokenizer est charg√© de pr√©traiter le texte pour en faire un tableau de chiffres qui servira d'entr√©e √† un mod√®le. De nombreuses r√®gles r√©gissent le processus de tokenisation, notamment la mani√®re de diviser un mot et le niveau auquel les mots doivent √™tre divis√©s (pour en savoir plus sur la tokenisation, consultez le [r√©sum√©](https://huggingface.co/docs/transformers/main/fr/./tokenizer_summary)). La chose la plus importante √† retenir est que vous devez instancier un tokenizer avec le m√™me nom de mod√®le pour vous assurer que vous utilisez les m√™mes r√®gles de tokenisation que celles avec lesquelles un mod√®le a √©t√© pr√©-entra√Æn√©.\n",
    "\n",
    "Chargez un tokenizer avec `AutoTokenizer` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passez votre texte au tokenizer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(\"We are very happy to show you the ü§ó Transformers library.\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le tokenizer retourne un dictionnaire contenant :\n",
    "\n",
    "* [input_ids](https://huggingface.co/docs/transformers/main/fr/./glossary#input-ids): la repr√©sentation num√©rique des tokens.\n",
    "* [attention_mask](https://huggingface.co/docs/transformers/main/fr/.glossary#attention-mask): indique quels tokens doivent faire l'objet d'une attention particuli√®re (plus particuli√®rement les tokens de remplissage).\n",
    "\n",
    "Un tokenizer peut √©galement accepter une liste de textes, et remplir et tronquer le texte pour retourner un √©chantillon de longueur uniforme :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_batch = tokenizer(\n",
    "    [\"We are very happy to show you the ü§ó Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "display(pt_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "Consultez le tutoriel [pr√©traitement](https://huggingface.co/docs/transformers/main/fr/./preprocessing) pour plus de d√©tails sur la tokenisation, et sur la mani√®re d'utiliser un `AutoImageProcessor`, un `AutoFeatureExtractor` et un `AutoProcessor` pour pr√©traiter les images, l'audio et les contenus multimodaux.\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ó Transformers fournit un moyen simple et unifi√© de charger des instances pr√©-entra√Æn√©es. Cela signifie que vous pouvez charger un `AutoModel` comme vous chargeriez un `AutoTokenizer`. La seule diff√©rence est de s√©lectionner l'`AutoModel` appropri√© pour la t√¢che. Pour une classification de texte (ou de s√©quence de textes), vous devez charger `AutoModelForSequenceClassification` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "Voir le [r√©sum√© de la t√¢che](https://huggingface.co/docs/transformers/main/fr/./task_summary) pour v√©rifier si elle est prise en charge par une classe `AutoModel`.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "Maintenant, passez votre √©chantillon d'entr√©es pr√©trait√©es directement au mod√®le. Il vous suffit de d√©compresser le dictionnaire en ajoutant `**` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_outputs = pt_model(**pt_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le mod√®le produit les activations finales dans l'attribut `logits`. Appliquez la fonction softmax aux `logits` pour r√©cup√©rer les probabilit√©s :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)\n",
    "print(pt_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "Tous les mod√®les ü§ó Transformers (PyTorch ou TensorFlow) produisent les tensors *avant* la fonction d'activation finale (comme softmax) car la fonction d'activation finale est souvent fusionn√©e avec le calcul de la perte. Les structures produites par le mod√®le sont des classes de donn√©es sp√©ciales, de sorte que leurs attributs sont autocompl√©t√©s dans un environnement de d√©veloppement. Les structures produites par le mod√®le se comportent comme un tuple ou un dictionnaire (vous pouvez les indexer avec un entier, une tranche ou une cha√Æne), auquel cas les attributs qui sont None sont ignor√©s.\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarder un mod√®le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous finetunez ou entrainez un mod√®le, vous pouvez le sauvegarder avec son tokenizer en utilisant `PreTrainedModel.save_pretrained()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_save_directory = \"./pt_save_pretrained\"\n",
    "tokenizer.save_pretrained(pt_save_directory)\n",
    "pt_model.save_pretrained(pt_save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque vous voulez r√©utiliser le mod√®le, rechargez-le avec `PreTrainedModel.from_pretrained()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
