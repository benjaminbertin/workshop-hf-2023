{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T√¢ches NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook pr√©sente les cas d'usages les plus fr√©quents en utilisant la librairie Transformers : question answering, sequence classification, named entity recognition etc.\n",
    "\n",
    "Ces exemples utilisent les pipelines et les classes Auto*. Ces classes vont cr√©er une instance d'un mod√®le en fonction d'un checkpoint, en s√©lectionnant automatiquement l'architecture du mod√®le.\n",
    "Pour en savoir plus, rendez-vous sur la documentation d'[AutoModel](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModel).\n",
    "\n",
    "Les checkpoints sont g√©n√©ralement pr√©-entrain√©s sur de grand jeu de donn√©es et fine-tun√©s pour une t√¢che sp√©cifique. Les limites sont :\n",
    "\n",
    "- Tous les mod√®les ne sont pas fine-tun√©s sur toutes les t√¢ches.\n",
    "- Les mod√®les ont √©t√© fine-tun√©s sur un jeu de donn√©es sp√©cifique. Ce jeu de donn√©es peut‚Åª√™tre diff√©rent de votre cas d'usage.\n",
    "\n",
    "Note : si un checkpoint demand√© n'est pas disponible pour une t√¢che donn√©e, seul le transformer de base sera charg√© et la partie du mod√®le sp√©cifique √† la t√¢che sera initialis√©e avec des poids al√©atoires (produisant des sorties al√©atoires)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classification de s√©quence consiste √† classifier des s√©quences selon plusieurs classes (voir le dataset [GLUE](https://huggingface.co/datasets/glue)).\n",
    "\n",
    "Ici nous utilisons un checkpoint pour d√©terminer si deux s√©quences sont une paraphrase l'une de l'autre. Le mod√®le requiert alors que nous lui fournissions une liste de dictionnaire de pair de texte. La sortie est `LABEL_0` et `LABEL_1` pour indiquer respectivement que ce n'est pas une paraphrase et que c'est une paraphrase :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "paraphrase_or_not = pipeline(\"text-classification\", model=\"bert-base-cased-finetuned-mrpc\")\n",
    "\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "text_pairs = [\n",
    "    {\"text\": sequence_0, \"text_pair\": sequence_1},\n",
    "    {\"text\": sequence_0, \"text_pair\": sequence_2}\n",
    "]\n",
    "results = paraphrase_or_not(text_pairs)\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessous le m√™me exemple utilisant les Auto* classes. Les √©tapes sont les suivantes :\n",
    "\n",
    "1. Cr√©er une instance de tokenizer et d'un mod√®le depuis un checkpoint.\n",
    "2. Cr√©er une s√©quence depuis deux phrases, en utilisant les s√©parateurs, le type de tokens et les masques d'attention attendus par le mod√®le\n",
    "3. Envoyer la s√©quence au mod√®le\n",
    "4. Calculer le softmax du r√©sultat pour obtenir des probabilit√©s\n",
    "5. Afficher le r√©sultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "# The tokenizer will automatically add any model specific separators (i.e. <CLS> and <SEP>) and tokens to\n",
    "# the sequence, as well as compute the attention masks.\n",
    "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "\n",
    "paraphrase_classification_logits = model(**paraphrase).logits\n",
    "not_paraphrase_classification_logits = model(**not_paraphrase).logits\n",
    "\n",
    "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "\n",
    "# Should be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should not be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'Extractive Question Answering consiste √† extraire une r√©ponse depuis un texte pour une question donn√©e. Un dataset usuel pour cette t√¢che est le [SQuAD dataset](https://huggingface.co/datasets/squad).\n",
    "\n",
    "Voici un exemple de pipeline qui permet de r√©aliser une t√¢che de question answering √† l'aide d'un mod√®le fine-tun√© sur SQuAD :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "\n",
    "context = r\"\"\"\n",
    "BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns an answer extracted from the text, a confidence score, alongside \"start\" and \"end\" values, which are the\n",
    "positions of the extracted answer in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = question_answerer(question=\"How many programming languages does BLOOM support?\", context=context)\n",
    "print(\n",
    "    f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = question_answerer(question=\"What can BLOOM do?\", context=context)\n",
    "print(\n",
    "    f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons faire de m√™me avec un mod√®le et un tokenizer :\n",
    "\n",
    "1. Cr√©er une instance d'un tokenizer et d'un mod√®le depuis un checkpoint\n",
    "2. D√©finir un contexte et des questions\n",
    "3. It√©rer sur les questions et construire une s√©quence avec le texte et la question courante, avec le s√©parateur du mod√®le, les bons types de tokens et le masque d'attention\n",
    "4. Inf√©rer la r√©ponse avec le mod√®le, la sortie contient des scores pour chaque token du contexte pour indiquer le d√©but et la fin de la r√©ponse √† extraire\n",
    "5. Calculer l'argmax des r√©sultats\n",
    "6. R√©cup√©rer le token de d√©but et de fin de la r√©ponse et les convertir en texte\n",
    "7. Afficher le r√©sultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = r\"\"\"\n",
    "ü§ó Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet‚Ä¶) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"How many pretrained models are available in ü§ó Transformers?\",\n",
    "    \"What does ü§ó Transformers provide?\",\n",
    "    \"ü§ó Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    # Get the most likely end of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "    )\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le Language modeling consiste √† entrainer un mod√®le sur un corpus documentaire (pouvant √™tre li√© √† un domaine pr√©cis). G√©n√©ralement, les mod√®les de transformers sont entrain√©s en utilisant le language modeling (BERT avec du  masked language modeling, GPT-2 avec du causal language modeling).\n",
    "\n",
    "Le Language modeling peut √™tre utile en dehors du pr√©-entrainement, notamment pour adapter un mod√®le √† un autre domaine. Par exemple en prenant un mod√®le entrain√© sur un vaste corpus documentaire et en l'adaptant sur un dataset d'articles scientifiques (voir [LysandreJik/arxiv-nlp](https://huggingface.co/lysandre/arxiv-nlp))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le Masked Language Modeling consiste √† cacher un ou plusieurs tolens dans une s√©quence, puis √† utiliser un mod√®le pour inf√©rer les tokens les plus appropri√©s pour les tokens masqu√©s. Cela permet au mod√®le de porter son attention aussi en amont et en aval du ou des tokens cach√©s. Ce type de mod√®le et l'entrainement associ√© permet de cr√©er un base robuste pour des t√¢ches en aval tel que l'Extractive Question Answering (voir [Lewis, Lui, Goyal et al.](https://arxiv.org/abs/1910.13461), part 4.2).\n",
    "\n",
    "Ci-dessous, un exemple de pipeline pour remplacer un masque dans une s√©quence :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This outputs the sequences with the mask filled, the confidence score, and the token id in the tokenizer vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(\n",
    "    unmasker(\n",
    "        f\"HuggingFace is creating a {unmasker.tokenizer.mask_token} that the community uses to solve NLP tasks.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessous, du masked language modeling en utilisant un mod√®le et un tokenizer :\n",
    "\n",
    "1. Cr√©er une instance du mod√®le et du tokenizer depuis un nom de checkpoint\n",
    "2. D√©finir une s√©quence avec un token cach√© (en utilisant `tokenizer.mask_token`)\n",
    "3. Encoder la s√©quence en une liste d'IDs et trouv√© la position du token masqu√© dans la liste\n",
    "4. R√©cup√©rer les pr√©dictions pour l'index du token masqu√©. Ces pr√©dictions contiennent un score pour chaque token du vocabulaire du mod√®le. Les scores les plus √©lev√©s correspondent aux tokens les plus probables\n",
    "5. R√©cup√©rer les 5 tokens les plus probables avec la fonction PyTorch `topk`\n",
    "6. Remplacer le masque par les tokens et afficher le r√©sultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-cased\")\n",
    "\n",
    "sequence = (\n",
    "    \"Distilled models are smaller than the models they mimic. Using them instead of the large \"\n",
    "    f\"versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "token_logits = model(**inputs).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "print(\"mask_token_logits shape\", mask_token_logits.shape)\n",
    "\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "torch.topk\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le Causal language modeling consiste √† pr√©dire le token le plus probable compl√©tant une s√©quence donn√©e. Ici le mod√®le ne portera son attention qu'aux tokens situ√©s en amont du masque. Ce type de mod√®le est utile pour les t√¢ches g√©n√©ratives.\n",
    "\n",
    "Ci-dessous, un exemple d'utilisation d'un mod√®le et d'un tokenizer en utilisant la m√©thode [top_k_top_p_filtering()](https://huggingface.co/docs/transformers/main/en/internal/generation_utils#transformers.top_k_top_p_filtering) pour r√©cup√©rer un le token le plus probable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, top_k_top_p_filtering\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "sequence = f\"Hugging Face is based in DUMBO, New York City, and\"\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# get logits of last hidden state\n",
    "next_token_logits = model(**inputs).logits[:, -1, :]\n",
    "\n",
    "# filter\n",
    "filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)\n",
    "\n",
    "# sample\n",
    "probs = nn.functional.softmax(filtered_next_token_logits, dim=-1)\n",
    "next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "generated = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "resulting_string = tokenizer.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This outputs a (hopefully) coherent next token following the original sequence, which in our case is the word *is* or\n",
    "*features*.\n",
    "\n",
    "In the next section, we show how [generation.GenerationMixin.generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) can be used to\n",
    "generate multiple tokens up to a specified length instead of one token at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La generation de texte consiste √† cr√©er une s√©quence de texte coh√©rente √† partir d'un contexte (aka prompt). L'exemple suivant montre l'utilisation d'un pipeline pour la g√©n√©ration de texte en utilisant GPT-2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_generator = pipeline(\"text-generation\")\n",
    "print(text_generator(\"As far as I am concerned, I will\", max_length=50, do_sample=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons g√©n√©r√© un texte de 50 tokens maximum. Le pipeline utilise la m√©thode [PreTrainedModel.generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) pour g√©n√©rer du texte. Les arguments par d√©faut de cette m√©thode peuvent √™tre √©cras√©s par les arguments du pipeline (`max_length` et `do_sample` par exemple).\n",
    "\n",
    "Ci-dessous, nous g√©n√©rons du texte avec le mod√®le `XLNet` en utilisant la m√©thode `generate()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "\n",
    "# Padding text helps XLNet with short prompts - proposed by Aman Rusia in https://github.com/rusiaaman/XLNet-gen#methodology\n",
    "PADDING_TEXT = \"\"\"In 1991, the remains of Russian Tsar Nicholas II and his family\n",
    "(except for Alexei and Maria) are discovered.\n",
    "The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
    "remainder of the story. 1883 Western Siberia,\n",
    "a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
    "Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
    "father initially slaps him for making such an accusation, Rasputin watches as the\n",
    "man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
    "the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
    "with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\"\n",
    "\n",
    "prompt = \"Today the weather is really nice and I am planning on \"\n",
    "inputs = tokenizer(PADDING_TEXT + prompt, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "prompt_length = len(tokenizer.decode(inputs[0]))\n",
    "outputs = model.generate(inputs, max_length=250, do_sample=True, top_p=0.95, top_k=60)\n",
    "generated = prompt + tokenizer.decode(outputs[0])[prompt_length + 1 :]\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La g√©n√©ration de texte est possible avec *GPT-2*, *OpenAi-GPT*, *CTRL*, *XLNet*, *Transfo-XL* et *Reformer*. Les mod√®les *XLNet* et *Transfo-XL* n√©cessitent g√©n√©ralement du padding pour fonctionner correctement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La reconnaissance d'entit√©s nomm√©es (Named Entity Recognition, NER) consiste √† classifier les tokens d'une s√©quence selon plusieurs classes. Par exemple, pour identifier une personne, une entreprise ou un lieu.\n",
    "\n",
    "Ci-dessous nous utilisons un pipeline de NER pour identifier les entit√©s suivantes :\n",
    "\n",
    "- O, Outside of a named entity\n",
    "- B-MIS, Beginning of a miscellaneous entity right after another miscellaneous entity\n",
    "- I-MIS, Miscellaneous entity\n",
    "- B-PER, Beginning of a person's name right after another person's name\n",
    "- I-PER, Person's name\n",
    "- B-ORG, Beginning of an organisation right after another organisation\n",
    "- I-ORG, Organisation\n",
    "- B-LOC, Beginning of a location right after another location\n",
    "- I-LOC, Location\n",
    "\n",
    "Ce mod√®le a √©t√© entrain√© sur le dataset CoNLL-2003."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_pipe = pipeline(\"ner\")\n",
    "\n",
    "sequence = \"\"\"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO,\n",
    "therefore very close to the Manhattan Bridge which is visible from the window.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This outputs a list of all words that have been identified as one of the entities from the 9 classes defined above.\n",
    "Here are the expected results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in ner_pipe(sequence):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note how the tokens of the sequence \"Hugging Face\" have been identified as an organisation, and \"New York City\",\n",
    "\"DUMBO\" and \"Manhattan Bridge\" have been identified as locations.\n",
    "\n",
    "Nous pouvons aussi utiliser les Auto* classes pour r√©aliser cette NER :\n",
    "\n",
    "1. Cr√©er une instance d'un mod√®le et d'un tokenizer depuis un checkpoint\n",
    "2. D√©finir un texte avec des entit√©s nomm√©es connues\n",
    "3. Encoder la s√©quence en tokens\n",
    "4. R√©cup√©rer les pr√©dictions, la sortie sera une distribution pour chacune des 9 classes du mod√®le. Transformer la sortie en r√©cup√©rant la classe la plus probable pour chaque token avec argmax\n",
    "6. Afficher la classe pr√©dite pour chaque token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = (\n",
    "    \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, \"\n",
    "    \"therefore very close to the Manhattan Bridge.\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "tokens = inputs.tokens()\n",
    "\n",
    "outputs = model(**inputs).logits\n",
    "predictions = torch.argmax(outputs, dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrairement au pipeline, la sortie contient une pr√©diction pour chaque token (classe \"O\" inclus).\n",
    "\n",
    "Les pr√©dictions sont un entier correspondant √† la classe pr√©dite. Nous pouvons utiliser `model.config.id2label` pour r√©cup√©rer le nom de la classe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, prediction in zip(tokens, predictions[0].numpy()):\n",
    "    print((token, model.config.id2label[prediction]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La Summarization consiste √† r√©sumer un document ou un article en un texte plus court.\n",
    "\n",
    "Ci-dessous un exemple de pipeline qui r√©sume un texte en utilisant le mod√®le Bart (entrain√© sur le dataset CNN / Daily Mail data set qui contient des articles de ces deux sources et des r√©sum√©s) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le pipeline de summarization utilisant la m√©thode `PreTrainedModel.generate()`, nous pouvons d√©finir certains param√®tres tel que `max_length` et `min_length` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons r√©aliser de la summarization en utilisant un mod√®le et son tokenizer, le processus est le suivant :\n",
    "\n",
    "1. Cr√©er une instance d'un mod√®le (tel que `Bart` ou `T5`) et d'un tokenizer pour un chackpoint\n",
    "2. D√©finir un text a r√©sumer\n",
    "3. Ajouter le pr√©fixe \"summarize: \" au texte\n",
    "4. Utiliser la m√©thode `PreTrainedModel.generate()`pour g√©n√©rer un r√©sum√©\n",
    "\n",
    "Dans cet exemple, nous utilisons le mod√®le T5 de Google, qui bien qu'entrain√© sur un vaste corpus (incluant CNN / Daily Mail) fournit de tr√®s bons r√©sultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "# T5 uses a max_length of 512 so we cut the article to 512 tokens.\n",
    "inputs = tokenizer(\"summarize: \" + ARTICLE, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons aussi utiliser un mod√®le g√©n√©ratif tel que T5 pour traduire des textes. Ce pipeline utilisant la m√©thode `PreTrainedModel.generate()` nous pouvons d√©finir une valeur pour le param√®tre `max_length` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_en_to_fr\")\n",
    "print(translator(\"Hugging Face is a technology company based in New York and Paris\", max_length=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour traduire un texte en utilisant un mod√®le et son tokenizer avec les Auto* classes, il faut :\n",
    "\n",
    "1. Cr√©er une instance d'un mod√®le et d'un tokenizer √† partir d'un checkpoint\n",
    "2. D√©finir un text a traduire\n",
    "3. Ajouter le pr√©fixe \"translate English to French: \" au texte\n",
    "4. Utiliser la m√©thode PreTrainedModel.generate()pour g√©n√©rer un r√©sum√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    \"translate English to French: Hugging Face is a technology company based in New York and Paris\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=40, num_beams=4, early_stopping=True)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero shot classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certains mod√®les peuvent r√©aliser de la classification sans entrainement (zero shot learning) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(model=\"facebook/bart-large-mnli\")\n",
    "pipe(\n",
    "    \"I have a problem with my iphone that needs to be resolved asap!\",\n",
    "    candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
