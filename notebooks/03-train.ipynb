{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation de Transformers et pyTorch\n",
    "! pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "torch.__version__, transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructions de mod√®les personnalis√©s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez modifier la configuration du mod√®le pour changer la fa√ßon dont un mod√®le est construit. La configuration sp√©cifie les attributs d'un mod√®le, tels que le nombre de couches ou de t√™tes d'attention. Vous partez de z√©ro lorsque vous initialisez un mod√®le √† partir d'une configuration personnalis√©e. Les attributs du mod√®le sont initialis√©s de mani√®re al√©atoire et vous devrez entra√Æner le mod√®le avant de pouvoir l'utiliser pour obtenir des r√©sultats significatifs.\n",
    "\n",
    "Commencez par importer `AutoConfig`, puis chargez le mod√®le pr√©-entra√Æn√© que vous voulez modifier. Dans `AutoConfig.from_pretrained()`, vous pouvez sp√©cifier l'attribut que vous souhaitez modifier, tel que le nombre de t√™tes d'attention :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "my_config = AutoConfig.from_pretrained(\"distilbert-base-uncased\", n_heads=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cr√©ez un mod√®le personnalis√© √† partir de votre configuration avec `AutoModel.from_config()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "my_model = AutoModel.from_config(my_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consultez le guide [Cr√©er une architecture personnalis√©e](https://huggingface.co/docs/transformers/main/fr/./create_a_model) pour plus d'informations sur la cr√©ation de configurations personnalis√©es."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer - une boucle d'entra√Ænement optimis√©e par PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous les mod√®les sont des [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) standard, vous pouvez donc les utiliser dans n'importe quelle boucle d'entra√Ænement typique. Bien que vous puissiez √©crire votre propre boucle d'entra√Ænement, ü§ó Transformers fournit une classe `Trainer` pour PyTorch, qui contient la boucle d'entra√Ænement de base et ajoute des fonctionnalit√©s suppl√©mentaires comme l'entra√Ænement distribu√©, la pr√©cision mixte, et plus encore.\n",
    "\n",
    "En fonction de votre t√¢che, vous passerez g√©n√©ralement les param√®tres suivants √† `Trainer` :\n",
    "\n",
    "1. Un `PreTrainedModel` ou un [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module):\n",
    "\n",
    "   ```py\n",
    "   >>> from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "   >>> model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "   ```\n",
    "\n",
    "2. `TrainingArguments` contient les hyperparam√®tres du mod√®le que vous pouvez changer comme le taux d'apprentissage, la taille due l'√©chantillon, et le nombre d'√©poques pour s'entra√Æner. Les valeurs par d√©faut sont utilis√©es si vous ne sp√©cifiez pas d'hyperparam√®tres d'apprentissage :\n",
    "\n",
    "   ```py\n",
    "   >>> from transformers import TrainingArguments\n",
    "\n",
    "   >>> training_args = TrainingArguments(\n",
    "   ...     output_dir=\"path/to/save/folder/\",\n",
    "   ...     learning_rate=2e-5,\n",
    "   ...     per_device_train_batch_size=8,\n",
    "   ...     per_device_eval_batch_size=8,\n",
    "   ...     num_train_epochs=2,\n",
    "   ... )\n",
    "   ```\n",
    "\n",
    "3. Une classe de pr√©traitement comme un tokenizer, un processeur d'images ou un extracteur de caract√©ristiques :\n",
    "\n",
    "   ```py\n",
    "   >>> from transformers import AutoTokenizer\n",
    "\n",
    "   >>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "   ```\n",
    "\n",
    "4. Chargez un jeu de donn√©es :\n",
    "\n",
    "   ```py\n",
    "   >>> from datasets import load_dataset\n",
    "\n",
    "   >>> dataset = load_dataset(\"rotten_tomatoes\")  # doctest: +IGNORE_RESULT\n",
    "   ```\n",
    "\n",
    "5. Cr√©ez une fonction qui transforme le texte du jeu de donn√©es en token :\n",
    "\n",
    "   ```py\n",
    "   >>> def tokenize_dataset(dataset):\n",
    "   ...     return tokenizer(dataset[\"text\"])\n",
    "   ```\n",
    "\n",
    "   Puis appliquez-la √† l'int√©gralit√© du jeu de donn√©es avec `map`:\n",
    "\n",
    "   ```py\n",
    "   >>> dataset = dataset.map(tokenize_dataset, batched=True)\n",
    "   ```\n",
    "\n",
    "6. Un `DataCollatorWithPadding` pour cr√©er un √©chantillon d'exemples √† partir de votre jeu de donn√©es :\n",
    "\n",
    "   ```py\n",
    "   >>> from transformers import DataCollatorWithPadding\n",
    "\n",
    "   >>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "   ```\n",
    "\n",
    "Maintenant, rassemblez tous ces √©l√©ments dans un `Trainer` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois que vous √™tes pr√™t, appelez la fonction `train()` pour commencer l'entra√Ænement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "Pour les t√¢ches - comme la traduction ou la g√©n√©ration de r√©sum√© - qui utilisent un mod√®le s√©quence √† s√©quence, utilisez plut√¥t les classes `Seq2SeqTrainer` et `Seq2SeqTrainingArguments`.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "Vous pouvez personnaliser le comportement de la boucle d'apprentissage en red√©finissant les m√©thodes √† l'int√©rieur de `Trainer`. Cela vous permet de personnaliser des caract√©ristiques telles que la fonction de perte, l'optimiseur et le planificateur. Consultez la documentation de `Trainer` pour savoir quelles m√©thodes peuvent √™tre red√©finies. \n",
    "\n",
    "L'autre moyen de personnaliser la boucle d'apprentissage est d'utiliser les [Callbacks](https://huggingface.co/docs/transformers/main/fr/./main_classes/callbacks). Vous pouvez utiliser les callbacks pour int√©grer d'autres biblioth√®ques et inspecter la boucle d'apprentissage afin de suivre la progression ou d'arr√™ter l'apprentissage plus t√¥t. Les callbacks ne modifient rien dans la boucle d'apprentissage elle-m√™me. Pour personnaliser quelque chose comme la fonction de perte, vous devez red√©finir le `Trainer` √† la place."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
